{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting individual car links from all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (134.0.6998.165) detected in PATH at /opt/homebrew/bin/chromedriver might not be compatible with the detected chrome version (135.0.7049.96); currently, chromedriver 135.0.7049.95 is recommended for chrome 135.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 1 ---\n"
     ]
    }
   ],
   "source": [
    " # Set up headless Chrome browser\n",
    "options = Options()\n",
    "#options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"window-size=1920,1080\")\n",
    "#options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "url = f\"https://www.carvana.com/cars\"\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Start on a specific page\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "result = []\n",
    "\n",
    "# Loop over pages\n",
    "NUM_PAGES = 2014\n",
    "for i in range(NUM_PAGES):\n",
    "    print(f\"--- Page {i+1} ---\")\n",
    "    \n",
    "    #scroll_to_bottom()\n",
    "    \n",
    "    # Optional: you can extract data here before clicking next\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Parse vehicle links from <script type=\"application/ld+json\">\n",
    "    json_scripts = soup.find_all(\"script\", type=\"application/ld+json\")\n",
    "    car_links = []\n",
    "\n",
    "    for script in json_scripts:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if data.get(\"@type\") == \"Vehicle\":\n",
    "                car_links.append(data.get(\"offers\", {}).get(\"url\", \"\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "    result.extend(car_links)\n",
    "    \n",
    "    try:\n",
    "        next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'li[data-qa=\"next-page\"]')))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "        time.sleep(1)\n",
    "        #driver.find_element(By.TAG_NAME, \"body\").click()  # unfocus any search box\n",
    "        #time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(\"Next button not found or not clickable:\", e)\n",
    "        break\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping individual car data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"columns.txt\", \"r\") as file:\n",
    "    existing_cols = [line.strip() for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique urls : 19396\n",
      "Scraped urls : 19397\n"
     ]
    }
   ],
   "source": [
    "# Read URLs from txt file into a list\n",
    "with open(\"unique_urls.txt\", \"r\") as file:\n",
    "    unique_urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Print the list\n",
    "print(f'Unique urls : {len(unique_urls)}')\n",
    "\n",
    "prev_result = pd.read_csv('car_data.csv', low_memory=False)\n",
    "scraped_urls = list(prev_result[~prev_result['bodyType'].isna()]['url'])\n",
    "\n",
    "print(f'Scraped urls : {len(scraped_urls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping car details: 100%|██████████| 19396/19396 [5:53:59<00:00,  1.10s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Set up headless Chrome browser\n",
    "auto_check = \"https://apik.carvana.io/merch/vdp/api/autocheck/v1/get?vehicleId=\"\n",
    "failed_url = []\n",
    "\n",
    "\n",
    "for i, car_url in tqdm(enumerate(unique_urls), total=len(unique_urls), desc=\"Scraping car details\"):\n",
    "#for car_url in unique_urls:\n",
    "    try: \n",
    "        if car_url not in scraped_urls:\n",
    "            options = Options()\n",
    "            #options.add_argument(\"--headless=new\")\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            options.add_argument(\"window-size=1920,1080\")\n",
    "            #options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "            driver.get(car_url)\n",
    "            time.sleep(3)\n",
    "            car_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            auto_check_url = auto_check + car_url[-7:]\n",
    "            driver.get(auto_check_url)\n",
    "            time.sleep(3)\n",
    "            auto_check_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            driver.quit()\n",
    "\n",
    "            image_tags = car_soup.find_all(\"img\", {\"data-qa\": \"carousel-item\"})\n",
    "            image_urls = [img['src'] for img in image_tags if 'src' in img.attrs]\n",
    "\n",
    "            json_scripts = car_soup.find_all(\"script\", type=\"application/json\")\n",
    "            for script in json_scripts:\n",
    "                try:\n",
    "                    json_data = json.loads(script.string)\n",
    "                    vehicle_details = json_data.get(\"props\", {}).get(\"pageProps\", {}).get(\"forProviders\", {}).get(\"forVehicleContext\", {}).get(\"vehicleDetails\", {})\n",
    "                    vehicle_details['url'] = car_url\n",
    "                    try:\n",
    "                        owner = auto_check_soup.find(\"span\", class_=\"box-title-owners\").find(\"span\").text\n",
    "                    except AttributeError:\n",
    "                        owner = \"1\"\n",
    "                    vehicle_details['owners'] = owner\n",
    "\n",
    "                    #vehicle_details['status'] = car_response.status_code\n",
    "                    df = pd.DataFrame([vehicle_details])\n",
    "                    #car_data = pd.concat([car_data, df], ignore_index=True)\n",
    "                    df = df.reindex(columns=existing_cols, fill_value=\"\")\n",
    "                    #df.to_csv(\"car_data.csv\", index=False, mode='a', header=False) # appends data to my existing file\n",
    "                    with open('car_data.csv', mode='a', newline='', encoding='utf-8') as f:\n",
    "                        df.to_csv(f, index=False, header=False)\n",
    "                        f.flush()\n",
    "\n",
    "                    #### scraping the image urls ####\n",
    "                    image_df = pd.DataFrame([{'make': df.loc[0,'make'], 'model': df.loc[0,'model'], 'image_url': url} for url in image_urls])\n",
    "                    with open('images.csv', mode='a', newline='', encoding='utf-8') as f:\n",
    "                        image_df.to_csv(f, index=False, header=False)\n",
    "                        f.flush()\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                \n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "    except Exception as e:\n",
    "        failed_url.append(car_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carvana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
